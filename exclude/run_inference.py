#!/usr/bin/env python3
"""
run_inference.py

RunPod GPU í™˜ê²½ì—ì„œ ì œì™¸ ëŒ€ìƒ ì‹ë³„ì íƒì§€ ëª¨ë¸ ì‹¤í–‰
- JSONL ë°ì´í„°ì…‹ ì…ë ¥ (instruction, input êµ¬ì¡°)
- í•™ìŠµ ì‹œ ì‚¬ìš©í•œ Alpaca í˜•ì‹ê³¼ ë™ì¼í•˜ê²Œ ì¶”ë¡ 
- íŒŒì¼ë³„ + ì „ì²´ ì‹ë³„ì ìˆ˜ì§‘
- ì¤‘ë‹¨ ì‹œ ì¬ê°œ ê¸°ëŠ¥
- í•´ì‹œ ê¸°ë°˜ ì²´í¬í¬ì¸íŠ¸
"""

import json
import os
import hashlib
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm
import argparse
from llama_cpp import Llama


class SensitiveIdentifierDetector:
    """Swift ì½”ë“œì—ì„œ ì œì™¸ ëŒ€ìƒ ì‹ë³„ì íƒì§€"""

    def __init__(self,
                 base_model_path: str,
                 lora_path: str,
                 n_ctx: int = 32768,
                 n_gpu_layers: int = -1,
                 checkpoint_file: str = "checkpoint/processed.jsonl"):
        """
        Args:
            base_model_path: ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ
            lora_path: LoRA ì–´ëŒ‘í„° ê²½ë¡œ
            n_ctx: ì»¨í…ìŠ¤íŠ¸ í¬ê¸° (ê¸°ë³¸ 32768, Phi-3-mini-128k ì§€ì›)
            n_gpu_layers: GPU ë ˆì´ì–´ ìˆ˜ (-1 = ì „ì²´)
            checkpoint_file: ì²˜ë¦¬ ì™„ë£Œëœ íŒŒì¼ ê¸°ë¡
        """
        self.checkpoint_file = checkpoint_file
        self.processed_hashes = self._load_checkpoint()

        print("=" * 60)
        print("ëª¨ë¸ ë¡œë”© ì¤‘...")
        print("=" * 60)
        print(f"Base model: {base_model_path}")
        print(f"LoRA adapter: {lora_path}")
        print(f"Context size: {n_ctx}")
        print(f"GPU layers: {n_gpu_layers if n_gpu_layers != -1 else 'ALL'}")

        try:
            self.model = Llama(
                model_path=base_model_path,
                lora_path=lora_path,
                n_ctx=n_ctx,
                n_gpu_layers=n_gpu_layers,
                n_threads=4,
                verbose=False,
                use_mmap=True,
                use_mlock=False,
            )
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def _compute_hash(self, instruction: str, input_data) -> str:
        """instruction + inputìœ¼ë¡œ í•´ì‹œ ìƒì„±

        Args:
            instruction: ì§€ì‹œì‚¬í•­
            input_data: ì…ë ¥ ë°ì´í„° (dict ë˜ëŠ” str)
        """
        if isinstance(input_data, dict):
            input_str = json.dumps(input_data, ensure_ascii=False, sort_keys=True)
        else:
            input_str = str(input_data)

        combined = f"{instruction}::{input_str}"
        return hashlib.sha256(combined.encode('utf-8')).hexdigest()

    def _load_checkpoint(self) -> Dict[str, Dict]:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ

        Returns:
            Dict[hash, result]: í•´ì‹œë¥¼ í‚¤ë¡œ, ê²°ê³¼ë¥¼ ê°’ìœ¼ë¡œ
        """
        if not os.path.exists(self.checkpoint_file):
            return {}

        processed = {}
        with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line)
                        processed[data['hash']] = data['result']
                    except:
                        continue

        if processed:
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {len(processed)}ê°œ í•­ëª© ì´ë¯¸ ì²˜ë¦¬ë¨")

        return processed

    def _save_checkpoint(self, content_hash: str, result: Dict):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)

        checkpoint_entry = {
            "hash": content_hash,
            "result": result
        }

        with open(self.checkpoint_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(checkpoint_entry, ensure_ascii=False) + '\n')

        self.processed_hashes[content_hash] = result

    def _should_process(self, instruction: str, input_data) -> tuple:
        """ì²˜ë¦¬ ì—¬ë¶€ í™•ì¸

        Args:
            instruction: ì§€ì‹œì‚¬í•­
            input_data: ì…ë ¥ ë°ì´í„° (dict ë˜ëŠ” str)

        Returns:
            (should_process: bool, cached_result: Dict or None)
        """
        current_hash = self._compute_hash(instruction, input_data)

        if current_hash in self.processed_hashes:
            return False, self.processed_hashes[current_hash]

        return True, None

    def _format_prompt(self, instruction: str, input_data) -> str:
        """í•™ìŠµ ì‹œ ì‚¬ìš©í•œ Alpaca í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            instruction: ì§€ì‹œì‚¬í•­
            input_data: ì…ë ¥ ë°ì´í„° (dict ë˜ëŠ” str)

        í˜•ì‹: ### Instruction:\n{inst}\n\n### Input:\n{inp}\n\n### Response:\n
        """
        inst = instruction.strip()

        if isinstance(input_data, dict):
            inp = json.dumps(input_data, ensure_ascii=False, indent=2)
        elif isinstance(input_data, str):
            inp = input_data.strip()
        else:
            inp = str(input_data)

        if inp:
            prompt = f"### Instruction:\n{inst}\n\n### Input:\n{inp}\n\n### Response:\n"
        else:
            prompt = f"### Instruction:\n{inst}\n\n### Response:\n"

        return prompt

    def detect_sensitive_identifiers(self, item: Dict[str, Any], metadata: Dict[str, str]) -> Dict[str, List[str]]:
        """ë‹¨ì¼ í•­ëª©ì—ì„œ ì œì™¸ ëŒ€ìƒ ì‹ë³„ì íƒì§€

        Args:
            item: {"instruction": "...", "input": {...}}
            metadata: {"source_file": "...", "file_path": "..."}

        Returns:
            {
                "source_file": "...",
                "sensitive_identifiers": {...},
                "reasoning": {...}
            }
        """
        instruction = item.get('instruction', '')
        input_data = item.get('input', {})

        should_process, cached_result = self._should_process(instruction, input_data)

        if not should_process:
            print(f"  ğŸ’¾ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©")
            return cached_result

        prompt = self._format_prompt(instruction, input_data)

        try:
            response = self.model.create_completion(
                prompt=prompt,
                max_tokens=20480,
                temperature=0.1,
                top_p=0.95,
                stop=["<|endoftext|>", "###"],
                echo=False
            )

            output = response['choices'][0]['text'].strip()

            finish_reason = response['choices'][0].get('finish_reason', '')
            if finish_reason == 'length':
                print(f"  âš ï¸ ì¶œë ¥ì´ max_tokensì— ì˜í•´ ì˜ë¦¼ (í† í° ë¶€ì¡±)")

            parsed_output = self._parse_output(output)

            result = {
                "source_file": metadata.get("source_file", "unknown"),
                "sensitive_identifiers": parsed_output.get("sensitive_identifiers", {}),
                "raw_output": output[:500]
            }

            content_hash = self._compute_hash(instruction, input_data)
            self._save_checkpoint(content_hash, result)

            return result

        except Exception as e:
            print(f"\nâš ï¸  ì—ëŸ¬ ë°œìƒ: {e}")
            return {
                "source_file": metadata.get("source_file", "unknown"),
                "sensitive_identifiers": {},
                "error": str(e)
            }

    def _parse_output(self, output: str) -> Dict:
        """ëª¨ë¸ ì¶œë ¥ì—ì„œ sensitive_identifiers ì¶”ì¶œ

        ì˜ˆìƒ í˜•ì‹:
        {
          "sensitive_identifiers": {
            "identifier1": {
              "reasoning": "..."
            },
            "identifier2": {
              "reasoning": "..."
            }
          }
        }
        """
        import re

        try:
            json_candidates = []
            depth = 0
            start_idx = -1

            for i, char in enumerate(output):
                if char == '{':
                    if depth == 0:
                        start_idx = i
                    depth += 1
                elif char == '}':
                    depth -= 1
                    if depth == 0 and start_idx != -1:
                        json_candidates.append(output[start_idx:i + 1])
                        start_idx = -1

            json_candidates.sort(key=len, reverse=True)

            for json_str in json_candidates:
                try:
                    json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
                    json_str = re.sub(r'//.*?$', '', json_str, flags=re.MULTILINE)

                    parsed = json.loads(json_str)

                    if "sensitive_identifiers" in parsed:
                        return parsed

                except json.JSONDecodeError:
                    continue

            return {"sensitive_identifiers": {}}

        except Exception as e:
            print(f"  âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {"sensitive_identifiers": {}}

    def _count_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì •"""
        try:
            tokens = self.model.tokenize(text.encode('utf-8'), add_bos=True)
            return len(tokens)
        except:
            return len(text) // 4

    def _filter_by_tokens(self, dataset: List[Dict], max_input_tokens: int) -> List[Dict]:
        """í† í° ìˆ˜ê°€ max_input_tokens ì´í•˜ì¸ í•­ëª©ë§Œ í•„í„°ë§

        Args:
            dataset: JSONL ë°ì´í„°ì…‹
            max_input_tokens: ìµœëŒ€ í—ˆìš© ì…ë ¥ í† í° ìˆ˜

        Returns:
            í•„í„°ë§ëœ ë°ì´í„°ì…‹
        """
        print(f"ğŸ” í† í° ìˆ˜ í•„í„°ë§ ì¤‘ (max_input_tokens={max_input_tokens})...")

        filtered = []
        skipped = []

        for item in tqdm(dataset, desc="í•„í„°ë§", unit="item"):
            instruction = item.get('instruction', '')
            input_data = item.get('input', {})

            prompt = self._format_prompt(instruction, input_data)
            token_count = self._count_tokens(prompt)

            if token_count <= max_input_tokens:
                filtered.append(item)
            else:
                skipped.append({
                    "source_file": item.get('metadata', {}).get('source_file', 'unknown'),
                    "tokens": token_count
                })

        print(f"\nğŸ“Š í•„í„°ë§ ê²°ê³¼:")
        print(f"  ì›ë³¸ ë°ì´í„°ì…‹: {len(dataset)}ê°œ")
        print(f"  í† í° ì´ˆê³¼ ìŠ¤í‚µ: {len(skipped)}ê°œ")
        print(f"  í•„í„°ë§ í›„: {len(filtered)}ê°œ")

        if skipped:
            print(f"\nâš ï¸  ìŠ¤í‚µëœ íŒŒì¼ (í† í° ìˆ˜ ë§ìŒ):")
            for s in skipped[:5]:
                print(f"    - {s['source_file']}: {s['tokens']} tokens")
            if len(skipped) > 5:
                print(f"    ... ì™¸ {len(skipped) - 5}ê°œ")

        return filtered

    def _format_time(self, seconds: float) -> str:
        """ì´ˆë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if seconds < 60:
            return f"{seconds:.0f}ì´ˆ"
        elif seconds < 3600:
            mins = seconds / 60
            return f"{mins:.0f}ë¶„"
        else:
            hours = seconds / 3600
            mins = (seconds % 3600) / 60
            return f"{hours:.0f}ì‹œê°„ {mins:.0f}ë¶„"

    def process_dataset(self,
                        dataset_path: str,
                        output_dir: str = "output",
                        max_input_tokens: int = 30000,
                        reset: bool = False) -> Dict:
        """ë°ì´í„°ì…‹ ì „ì²´ ì²˜ë¦¬

        Args:
            dataset_path: JSONL ë°ì´í„°ì…‹ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            max_input_tokens: ìµœëŒ€ ì…ë ¥ í† í° ìˆ˜
            reset: ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™” ì—¬ë¶€

        Returns:
            ì²˜ë¦¬ í†µê³„
        """
        if reset and os.path.exists(self.checkpoint_file):
            os.remove(self.checkpoint_file)
            self.processed_hashes = {}
            print("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

        os.makedirs(output_dir, exist_ok=True)

        per_file_output = os.path.join(output_dir, "per_file_results.jsonl")
        all_identifiers_output = os.path.join(output_dir, "all_identifiers.json")
        summary_output = os.path.join(output_dir, "summary.json")

        print("\n" + "=" * 60)
        print("ë°ì´í„°ì…‹ ì²˜ë¦¬ ì‹œì‘")
        print("=" * 60)

        with open(dataset_path, 'r', encoding='utf-8') as f:
            dataset = [json.loads(line) for line in f if line.strip()]

        print(f"\nğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ: {len(dataset)}ê°œ í•­ëª©")

        filtered_dataset = self._filter_by_tokens(dataset, max_input_tokens)

        need_processing = []
        for item in filtered_dataset:
            instruction = item.get('instruction', '')
            input_data = item.get('input', {})
            should_process, _ = self._should_process(instruction, input_data)
            if should_process:
                need_processing.append(item)

        total_items = len(filtered_dataset)

        print(f"\nğŸ“ ì²˜ë¦¬ ìƒíƒœ:")
        print(f"  í•„í„°ë§ í›„ í•­ëª©: {len(filtered_dataset)}ê°œ")
        print(f"  ì´ë¯¸ ì²˜ë¦¬ë¨: {len(filtered_dataset) - len(need_processing)}ê°œ")
        print(f"  ì²˜ë¦¬ í•„ìš”: {len(need_processing)}ê°œ")

        if max_input_tokens:
            max_tokens_display = f"~{max_input_tokens} tokens"
        else:
            max_tokens_display = "ë¬´ì œí•œ"
        print(f"  ìµœëŒ€ ì…ë ¥ í† í°: {max_tokens_display}")

        per_file_results = []
        all_sensitive_identifiers = {}

        if len(need_processing) == 0:
            print("\nâœ… ëª¨ë“  í•­ëª©ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            return self._load_existing_results(output_dir)

        import time
        start_time = time.time()
        processed_count = 0

        with tqdm(total=len(need_processing), desc="Processing",
                  unit="file", ncols=100) as pbar:
            for item in need_processing:
                item_start = time.time()

                metadata = item.get('metadata', {})

                result = self.detect_sensitive_identifiers(item, metadata)
                per_file_results.append(result)

                sensitive_ids = result.get("sensitive_identifiers", {})
                for identifier in sensitive_ids.keys():
                    all_sensitive_identifiers[identifier] = all_sensitive_identifiers.get(identifier, 0) + 1

                processed_count += 1

                elapsed = time.time() - start_time
                if processed_count > 0:
                    remaining_items = len(need_processing) - processed_count
                    avg_time_per_item = elapsed / processed_count
                    eta_seconds = remaining_items * avg_time_per_item
                    eta_str = self._format_time(eta_seconds)
                    pbar.set_postfix({
                        'ETA': eta_str,
                        'avg': f'{avg_time_per_item:.1f}s/file',
                        'ids': len(all_sensitive_identifiers)
                    })

                pbar.update(1)

                if processed_count % 10 == 0:
                    self._save_results(per_file_results, all_sensitive_identifiers,
                                       per_file_output, all_identifiers_output)

        self._save_results(per_file_results, all_sensitive_identifiers,
                           per_file_output, all_identifiers_output)

        total_time = time.time() - start_time
        time_str = self._format_time(total_time)

        total_identifiers_count = sum(
            len(r.get("sensitive_identifiers", {}))
            for r in per_file_results
        )

        summary = {
            "total_files": total_items,
            "processed_files": len(per_file_results),
            "total_sensitive_identifiers": total_identifiers_count,
            "unique_sensitive_identifiers": len(all_sensitive_identifiers),
            "processing_time": total_time,
            "avg_time_per_file": total_time / total_items if total_items > 0 else 0,
            "output_files": {
                "per_file": per_file_output,
                "all_identifiers": all_identifiers_output,
                "summary": summary_output
            }
        }

        with open(summary_output, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        print("\n" + "=" * 60)
        print("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print("=" * 60)
        print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {time_str}")
        print(f"ğŸ“Š í‰ê·  ì²˜ë¦¬ ì†ë„: {summary['avg_time_per_file']:.1f}ì´ˆ/íŒŒì¼")
        print(f"ğŸ“ ì²˜ë¦¬ëœ íŒŒì¼: {summary['processed_files']}ê°œ")
        print(f"ğŸ” ì´ ë¯¼ê° ì‹ë³„ì (ì¤‘ë³µ í¬í•¨): {summary['total_sensitive_identifiers']}ê°œ")
        print(f"âœ¨ ê³ ìœ  ë¯¼ê° ì‹ë³„ì: {summary['unique_sensitive_identifiers']}ê°œ")
        print(f"\nğŸ“‚ ì¶œë ¥ íŒŒì¼:")
        print(f"  â€¢ {per_file_output}")
        print(f"  â€¢ {all_identifiers_output}")
        print(f"  â€¢ {summary_output}")

        return summary

    def _save_results(self, per_file_results: List[Dict],
                      all_identifiers: Dict[str, int],
                      per_file_output: str,
                      all_identifiers_output: str):
        """ê²°ê³¼ ì €ì¥"""
        with open(per_file_output, 'w', encoding='utf-8') as f:
            for result in per_file_results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')

        sorted_identifiers = dict(
            sorted(all_identifiers.items(), key=lambda x: x[1], reverse=True)
        )

        all_ids_data = {
            "sensitive_identifiers": sorted_identifiers,
            "total_unique": len(sorted_identifiers),
            "total_count": sum(sorted_identifiers.values())
        }

        with open(all_identifiers_output, 'w', encoding='utf-8') as f:
            json.dump(all_ids_data, f, ensure_ascii=False, indent=2)

    def _load_existing_results(self, output_dir: str) -> Dict:
        """ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ"""
        summary_path = os.path.join(output_dir, "summary.json")
        if os.path.exists(summary_path):
            with open(summary_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}


def main():
    parser = argparse.ArgumentParser(description='ì œì™¸ ëŒ€ìƒ ì‹ë³„ì íƒì§€ ëª¨ë¸ ì‹¤í–‰')
    parser.add_argument('--dataset', type=str, default='dataset.jsonl',
                        help='ì…ë ¥ JSONL ë°ì´í„°ì…‹ ê²½ë¡œ')
    parser.add_argument('--base_model', type=str, default='models/base_model.gguf',
                        help='ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ (GGUF)')
    parser.add_argument('--lora', type=str, default='models/lora.gguf',
                        help='LoRA ì–´ëŒ‘í„° ê²½ë¡œ (GGUF)')
    parser.add_argument('--output', type=str, default='output',
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--ctx', type=int, default=32768,
                        help='ì»¨í…ìŠ¤íŠ¸ í¬ê¸°')
    parser.add_argument('--max_input_tokens', type=int, default=30000,
                        help='ìµœëŒ€ ì…ë ¥ í† í° ìˆ˜ (0 = ë¬´ì œí•œ)')
    parser.add_argument('--gpu_layers', type=int, default=-1,
                        help='GPU ë ˆì´ì–´ ìˆ˜ (-1 = ì „ì²´)')
    parser.add_argument('--reset', action='store_true',
                        help='ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™”')

    args = parser.parse_args()

    detector = SensitiveIdentifierDetector(
        base_model_path=args.base_model,
        lora_path=args.lora,
        n_ctx=args.ctx,
        n_gpu_layers=args.gpu_layers
    )

    detector.process_dataset(
        dataset_path=args.dataset,
        output_dir=args.output,
        max_input_tokens=args.max_input_tokens,
        reset=args.reset
    )

    print("\nâœ¨ í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    main()